{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac0a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scraped page 8 successfully!\n",
      "‚úÖ Scraped page 3 successfully!\n",
      "‚úÖ Scraped page 1 successfully!\n",
      "‚úÖ Scraped page 6 successfully!\n",
      "‚úÖ Scraped page 7 successfully!\n",
      "‚úÖ Scraped page 5 successfully!\n",
      "‚úÖ Scraped page 4 successfully!\n",
      "‚úÖ Scraped page 9 successfully!\n",
      "‚úÖ Scraped page 2 successfully!\n",
      "‚úÖ Scraped page 10 successfully!\n",
      "‚úÖ Scraped page 11 successfully!\n",
      "‚úÖ Scraped page 14 successfully!\n",
      "‚úÖ Scraped page 12 successfully!\n",
      "‚úÖ Scraped page 13 successfully!\n",
      "‚úÖ Scraped page 15 successfully!\n",
      "‚úÖ Scraped page 16 successfully!\n",
      "‚úÖ Scraped page 17 successfully!\n",
      "‚úÖ Scraped page 19 successfully!\n",
      "‚úÖ Scraped page 18 successfully!\n",
      "‚úÖ Scraped page 20 successfully!\n",
      "‚úÖ Scraped page 21 successfully!\n",
      "üíæ Data saved to: E:\\DHP\\Group_Project\\amazon_product_mobiles_data.xlsx (Total rows: 207)\n",
      "üíæ Data saved to: E:\\DHP\\Group_Project\\amazon_product_mobiles_data.xlsx (Total rows: 427)\n",
      "üíæ Data saved to: E:\\DHP\\Group_Project\\amazon_product_mobiles_data.xlsx (Total rows: 449)\n",
      "üéâ Data scraping completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import random\n",
    "\n",
    "# ‚úÖ Path to save Excel file\n",
    "save_path = r\"E:\\DHP\\Group_Project\\amazon_product_mobiles_data.xlsx\"\n",
    "\n",
    "# ‚úÖ Shared data storage (processed in batches)\n",
    "all_data = []\n",
    "lock = Lock()  # Thread safety\n",
    "\n",
    "# ‚úÖ User input for page range\n",
    "start_page = int(input(\"Enter the starting page number: \"))\n",
    "end_page = int(input(\"Enter the ending page number: \"))\n",
    "\n",
    "# ‚úÖ Number of threads for multithreading (8 cores)\n",
    "NUM_THREADS = 8\n",
    "MAX_RETRIES = 7  # Retry attempts in case of failure\n",
    "SAVE_INTERVAL = 10  # Save every 10 products\n",
    "\n",
    "# Rotate User-Agent list to avoid getting blocked\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Firefox/89.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:35.0) Gecko/20100101 Firefox/35.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Safari/537.36 Edge/91.0.864.59\"\n",
    "]\n",
    "\n",
    "def scrape_page(page_number):\n",
    "    \"\"\"Scrapes a single Amazon page with retries and returns extracted data.\"\"\"\n",
    "    site = f\"https://www.amazon.in/s?k=mobiles&crid=2HHJVMGRXCC2I&qid=1744563410&sprefix=mobiles%2Caps%2C783&xpid=k4m8Obu5IevXA&ref=sr_pg_{page_number}\"\n",
    "    \n",
    "    delay = 3  # Start with a higher delay to avoid rate limits\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(USER_AGENTS)\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = requests.get(site, timeout=30, headers=headers)  # ‚¨Ü Increased timeout to avoid read timeouts\n",
    "            if response.status_code == 503:  # Service Unavailable (Amazon is blocking requests)\n",
    "                wait_time = delay + random.uniform(1, 3)  # Randomized backoff\n",
    "                print(f\"‚ö†Ô∏è Service unavailable on page {page_number}, retrying in {wait_time:.2f}s (Attempt {attempt}/{MAX_RETRIES})...\")\n",
    "                time.sleep(wait_time)\n",
    "                delay *= 2  # Exponential backoff\n",
    "                continue  # Retry request\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to fetch page {page_number}: {response.status_code}\")\n",
    "                return []\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            products = soup.find_all(\"div\", class_=\"s-main-slot\")[0].find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "            page_data = []\n",
    "            for product in products:\n",
    "                # Extract Product Name using the <span> tag you provided\n",
    "                title = product.find(\"span\")\n",
    "                if title:\n",
    "                    title = title.get_text()  # Get the text inside the <span> tag\n",
    "\n",
    "                # Extract Price\n",
    "                price = product.find(\"span\", class_=\"a-price-whole\")\n",
    "                if price:\n",
    "                    price = price.get_text()\n",
    "\n",
    "                # Extract Rating\n",
    "                rating = product.find(\"span\", class_=\"a-icon-alt\")\n",
    "                if rating:\n",
    "                    rating = rating.get_text()\n",
    "\n",
    "                # Extract Product URL\n",
    "                link = product.find(\"a\", class_=\"a-link-normal\")\n",
    "                if link:\n",
    "                    product_url = \"https://www.amazon.in\" + link.get(\"href\")\n",
    "\n",
    "                page_data.append([title, price, rating, product_url])\n",
    "\n",
    "            print(f\"‚úÖ Scraped page {page_number} successfully!\")\n",
    "            return page_data  # Return extracted data\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Error on page {page_number} (attempt {attempt}/{MAX_RETRIES}): {e}\")\n",
    "            time.sleep(delay + random.uniform(1, 3))  # Randomized retry delay\n",
    "            delay *= 2  # Exponential backoff\n",
    "\n",
    "    print(f\"‚ùå Skipping page {page_number} after {MAX_RETRIES} failed attempts.\")\n",
    "    return []  # Return empty list if all retries fail\n",
    "\n",
    "def save_to_excel():\n",
    "    \"\"\"Efficiently appends new data to the Excel file in batches.\"\"\"\n",
    "    with lock:\n",
    "        if not all_data:\n",
    "            return\n",
    "\n",
    "        df = pd.DataFrame(all_data, columns=[\"Product Name\", \"Price\", \"Rating\", \"Product URL\"])\n",
    "\n",
    "        if os.path.exists(save_path):\n",
    "            existing_df = pd.read_excel(save_path)\n",
    "            df = pd.concat([existing_df, df], ignore_index=True)  # Append new data\n",
    "\n",
    "        df.to_excel(save_path, index=False)\n",
    "        print(f\"üíæ Data saved to: {save_path} (Total rows: {len(df)})\")\n",
    "\n",
    "        # ‚úÖ Clear memory after saving\n",
    "        all_data.clear()\n",
    "\n",
    "# ‚úÖ Parallel scraping with ThreadPoolExecutor\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "    future_to_page = {executor.submit(scrape_page, i): i for i in range(start_page, end_page + 1)}\n",
    "\n",
    "    for i, future in enumerate(as_completed(future_to_page), start=1):\n",
    "        result = future.result()\n",
    "        with lock:\n",
    "            all_data.extend(result)\n",
    "\n",
    "        # ‚úÖ Save every 10 products\n",
    "        if i % SAVE_INTERVAL == 0 and all_data:\n",
    "            save_to_excel()\n",
    "\n",
    "# ‚úÖ Final save when all pages are done\n",
    "if all_data:\n",
    "    save_to_excel()\n",
    "\n",
    "print(\"üéâ Data scraping completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
